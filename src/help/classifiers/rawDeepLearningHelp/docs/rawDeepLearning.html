<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<link href="../../../pamHelpStylesheet.css" type="text/css"
	rel="STYLESHEET">
<title>PAMGuard Deep Learning Module - Overview</title>

</head>
<body>
	<h1 id="PAMGuard-s-deep-learning-module">PAMGuard&#39;s Deep
		Learning Module</h1>
	<h2 id="overview">Overview</h2>

	<p>PAMGuard&#39;s deep learning module allows users to deploy a
		large variety of deep learning models natively in PAMGuard. It is core
		module, fully integrated into PAMGuard&#39;s display and data
		management system and can be used in real time or for post processing
		data. It can therefore be used as a classifier for almost any acoustic
		signal and can integrate into multiple types of acoustic analysis
		workflows, for example post analysis of recorder data or used as part
		of real time localisation workflow.</p>
		
	<br>
	<h3 id="how-it-works">How it works</h3>
	<p>The deep learning module accepts raw data from different types
		of data sources, e.g. from the Sound Acquisition module, clicks and
		clips. It segments data into equal sized chunks with a specified
		overlap. Each chunk is passed through a set of transforms which
		convert the data into a format which is accepted by the specified deep
		learning model. These transforms are either manually set up by the
		user or, if a specific type of framework has been used to train a deep
		learning model, then can be automatically set up by PAMGuard.
		Currently there are three implemented frameworks</p>
	<p align="center">
		<img width="900" height="370"
			src="images/deep_learning_module_process.png">
	</p>

	<p>
		<em>A diagram of how the deep learning module works in PAMGuard.
			An input waveform is segmented into chunks. A series of transforms
			are applied to each chunk creating the input for the deep learning
			model. The transformed chunks are sent to the model. The results from
			the model are saved and can be viewed in real time (e.g. mitigation)
			or in post processing (e.g. data from SoundTraps).</em>
	</p>
	<br>
	<h3 id="generic-model">Generic Model</h3>
	<p>
		A generic model allows a user to load any model compatible with the <a
			href="https://djl.ai/">djl</a> (PyTorch (JIT), Tenserflow, ONXX)
		library and then manually set up a series of transforms using
		PAMGuard&#39;s transform library. It is recommended that users use an
		existing framework instead of a generic model as these models will
		automatically generate the required transforms.
	</p>
	<br>
	<h3 id="animalspot">AnimalSpot</h3>
	<p>
		<a href="https://github.com/ChristianBergler/ANIMAL-SPOT">ANIMAL-SPOT</a>
		is a deep learning based framework which was initially designed for <a
			href="(https://github.com/ChristianBergler/ORCA-SPOT">killer
			whale sound detection</a>) in noise heavy underwater recordings (see <a
			href="https://www.nature.com/articles/s41598-019-47335-w">Bergler
			et al. (2019)</a>). It has now been expanded to a be species independent
		framework for training acoustic deep learning models using <a
			href="https://pytorch.org/">PyTorch</a> and Python. Imported
		AnimalSpot models will automatically set up their own data transforms
		and output classes.
	</p>
	<br>
	<h3 id="ketos">Ketos</h3>
	<p>
		<a href="https://meridian.cs.dal.ca/2015/04/12/ketos/">Ketos</a> is an
		acoustic deep learning framework based on Tensorflow and developed by
		<a href="https://meridian.cs.dal.ca/">MERIDIAN</a>. It has excellent
		resources and tutorials and Python libraries can be installed easily
		via pip. Imported Ketos model will automatically set up their own data
		transforms and output classes.
	</p>
	<h2 id="creating-an-instance-of-the-module">Creating an instance
		of the module</h2>
	<p>
		The module can be added from the <em>File&gt; Add modules &gt;
			Classifier &gt; Raw deep learning classifier</em> menu or by right
		clicking in the data model. More than one instance of the module can
		be added if multiple deep learning models are required.
	</p>
	<h2 id="module-settings">Module settings</h2>
	<p>
		The module settings are opened by selecting the <em>Settings &gt;
			Raw deep learning classifier</em> menu. The main settings pane is shown
		below and is split into three sections, <em>Raw Sound Data</em>, <em>Segmentation</em>
		and <em>Deep Learning Model</em>
	</p>
	<p align="center">
		<img width="700" height="630"
			src="images/deep_leanring_module_help.png">
	</p>

	<p>
		<em>The main settings pane for the deep learning module with
			descriptions</em>
	</p>
	<h3 id="raw-sound-data">Raw Sound Data</h3>
	<p>The deep learning module accepts any raw data source i.e., any
		data source that contains raw waveform data.</p>
	<p>If the data is continuous, e.g. from the Sound Acquisition
		module then deep learning detections are saved to PAMGuard&#39;s data
		management system if they pass a user defined prediction threshold.
		The raw waveform data for segments which pass prediction threshold is
		saved and the detection is annotated with the deep prediction results.
	</p>
	<p>If the data source is an existing detection data stream, e.g.
		clicks or clips, then the deep learning results are saved as an
		annotation attached each detection. The data is segmented in exactly
		the same way as continuous data and thus, depending on the length of
		raw data within the detection, there can be more than one prediction
		per detection.</p>
	<p>Channel grouping controls are used to arrange channels into
		groups. Channels in the same group are saved together for downstream
		processes. So, for example if channels 0 and 2 are in a group, then
		the raw waveform data from both channel 0 and 2 will be saved and can
		be used in downstream processes, e.g., for localisation.</p>
	<h3 id="segmentation">Segmentation</h3>
	<p>
		The segmentation section defines how the raw data is segmented. Some
		deep learning models require a specific segment size and others can be
		run with different segment sizes. The <em>Window Length</em> is the
		size of the segment in samples. The <em>Hop Length</em> is the overlap
		(from the start of the segment) in samples. A <em>Hop Length</em>
		which is the same as the segment length means no overlap. If a
		prediction passes threshold, then the raw data from segments is saved
		to PAMGuard binary files. If concurrent segments pass a prediction
		threshold, then they are saved as one data unit. The <em>Max.
			re-merge</em> is the maximum number of segments that can form a single
		data unit before a new data unit is automatically created.
	</p>
	<h3 id="deep-learning-model">Deep Learning Model</h3>
	<p>The deep learning model section is used to select the deep
		learning model. The drop down menu is used to select the framework the
		model is from e.g. Generic model. Note that each model type has a
		unique user interface which appears just below the drop down menu -
		currently these all look fairly similar.</p>
	<p>All frameworks require a model file to be selected using the
		browse button (File icon). A wait icon will appear and the model will
		be loaded. If the deep learning model loading is successful then the
		filename of the model will appear (e.g. saved_model.pb)</p>
	<p>
		<em>Note: when a model is first loaded, the computer must be
			connected to the internet as PAMGuard will download the correct
			libraries for the computer to open the specific model. On Windows
			machine these libraries are found in a hidden folder called ai.djl.
			in the user account folder.</em>
	</p>
	<p>Once the model has loaded there some unique options depending on
		the currently selected framework.</p>
	<h4 id="generic-model">Generic Model</h4>
	<p>
		A generic model must be set up via the <em>Advanced</em> menu button.
	</p>
	<p align="center">
		<img width="700" height="700"
			src="images/advanced_settings_generic_1.png">
	</p>

	<p>
		<em>Before a sound segment can be classified it must be converted
			into a format suitable for the deep learning model. This is achieved
			by a list of <em>transforms</em> which convert a raw sound data into
			an appropriate format. Usually this involves converting to a
			spectrogram image and then performing a series of noise reductions
			and interpolation step. For the generic model users either have to
			manually add transforms and input the correct settings for each, or
			load a transforms *.pgtr setting file
		</em>
	</p>
	<p>
		The <em>Model Transforms</em> tab in the advanced menu pane allows a
		user to set up a set of transforms. The <em>Add transfrom</em> +
		button adds a transforms and these can be dragged in order using the
		drag handles on the left of each transform. Each transform has
		it&#39;s own settings pane which can be expanded to show transform
		specific settings. The bottom of the advanced settings pane shows a
		preview of the data that will be input into the deep learning model,
		including the shape of the input data e.g. a 100x50 image.
	</p>
	<p align="center">
		<img width="700" height="700"
			src="images/advanced_settings_generic_2.png">
	</p>

	<p>
		<em>The Model Settings tab allows the model inputs and outputs to
			be defined</em>
	</p>
	<p>
		The <em>Model Settings</em> tab allows the model input shape and
		output shape/classes to be defined. Most models will have metadata on
		the input and output data and these can be set by selecting the <em>Use
			default model shape</em> and <em>Use default model out</em> switches
		respectively. Otherwise, the input and output shape and the output
		classes must be defined manually
	</p>
	<p>The import and export buttons on the bottom of the advanced
		settings pane can be used to export and import settings for the
		generic model. This means that users do not have to manually set up
		transforms and input and output data whenever settings up a new
		PAMGuard data model and allow easier sharing of classifiers amongst
		researchers.</p>
	<h4 id="animalspot-and-ketos-models">AnimalSpot and Ketos models</h4>
	<p>
		If using an AnimalSpot or Ketos model then all transforms are
		automatically set up. The transforms can be viewed and altered via the
		Advanced menu button but in the majority of cases these settings
		should not be used. It is advisable to select &quot;Use default
		segment length&quot; to change the <em>Window length</em> to the
		default for the selected model. Note that this is often necessary for
		Ketos models but usually not a requirement for AnimalSpot models.
	</p>
	<p align="center">
		<img width="700" height="700"
			src="images/advanced_settings_animalspot_1.png">
	</p>

	<p>
		<em>An AnimalSpot or Ketos model will automatically create a list
			of transforms with the appropriate settings. These is no need to use
			the advanced pane but it is there in case users wish to change
			transform settings for some reason</em>
	</p>
	<h2 id="running">Running</h2>
	<h3 id="real-time">Real time</h3>
	<p>In real time, the deep learning model runs automatically when
		processing starts. A warning will appear if there are issues with the
		model and/or it cannot cope with real time speeds.</p>
	<h3 id="viewer-mode">Viewer Mode</h3>
	<p>
		The deep learning module can be re-run on <em>detector</em> data (e.g.
		click or clip detections) in PAMGuard <em>viewer</em> mode. Detections
		can be reclassified by selecting the <em>Settings &gt; Raw Deep
			Learning Classifier &gt; Reclassify detections</em>. Select the data range
		in the reprocessing dialog e.g. Loaded Data and select <em>Start</em>.
		Detections without a deep learning annotation will have one added and
		detections with an existing annotation will have it overwritten.
	</p>
	<h2 id="viewing-and-exporting-results">Viewing and exporting
		results</h2>
	<p>Output from the deep learning module can be viewed in PAMGuard
		viewer mode, or extracted from binary files using MATLAB or R.</p>
	<h3 id="PAMGuard-viewer-mode">PAMGuard viewer mode</h3>
	<p>Detections form continuous raw data are shown in the datagram in
		the same way as all data streams in PAMGuard.</p>
	<p>The Time base display FX is best way to view detailed data
		outputs from the deep learning algorithm. The time base display can
		display almost all data types in PAMGuard on a large variety of
		different data axis. For example, click detections can be displayed on
		an amplitude, bearing, ICI, waveform and/or frequency axis. Deep
		learning detections (i.e. data units which have been saved from raw
		data using the deep learning detector) can be displayed on the time
		base display in the same way as many other detections and in addition,
		there is a symbol manager options which allows the deep learning
		detections or other detections which have been classified by the deep
		learning module to be coloured by prediction class. This means that a
		manual analyst can quickly navigate to detections with high prediction
		values for a certain class. Hovering over or right clicking on a data
		unit in the time display and selecting the information button, will
		show the data unit’s metadata, including the prediction values for all
		output classes from the deep learning model.</p>
	<p align="center">
		<img src="images/bat_time_base_display.png">
	</p>

	<p>
		<em>An example click detection module output coloured by deep
			learning annotations. Click detections are annotated with the results
			from the deep learning module. The symbol manager in the time base
			display can be used to colour the clicks by the prediction for a
			selected class</em>
	</p>
	<p>Other displays also show outputs from the deep learning module.
		Hovering over data units in the click display will, for example, show
		deep learning prediction values. The spectrogram will also show deep
		learning detections as translucent blue boxes (these must be selected
		in the right click menu).</p>
	<h3 id="matlab">MATLAB</h3>
	<p>The easiest way to export to MATLAB is to select the desired
		units in the time base display, right click and select the MATLAB
		icon. Data units will be exported to a .mat file as list of structures
		which is then saved to the clipboard. This file can be saved and then
		dragged into MATLAB to open.</p>
	<p>
		Where it is necessary to further analyse large datasets produced by
		PAMGuard, there is a MATLAB-PAMGuard library which can directly import
		the binary files which store PAMGaurd detection data. The library is
		simple to use with the primary function being
		<code>loadPAMGuardBinaryFile.m.</code>
		This will load any binary file type (e.g. clicks, whistles, deep
		learning detections) and return a list of data structures with the
		detection data. The structures include annotations where deep learning
		predictions are stored.
	</p>
	<p>Here is a simple example loading up all the deep learning
		detections for a right whale classifier.</p>
	<pre>
		<code class="lang-matlab">% <span class="hljs-keyword">the</span> <span
				class="hljs-built_in">folder</span> containing PAMGuard binary <span
				class="hljs-built_in">files</span>
<span class="hljs-built_in">folder</span> = <span class="hljs-string">'/Users/me/right_whale_project_1/PAMBinary/'</span>; 

 %<span class="hljs-built_in">load</span> all <span class="hljs-keyword">the</span> detections <span
				class="hljs-keyword">in</span> <span class="hljs-keyword">the</span> <span
				class="hljs-built_in">folder</span>
dldetections = loadPAMGuardBinaryFolder(<span class="hljs-built_in">folder</span>, <span
				class="hljs-string">'Deep_Learning_Classifier_Raw_Deep_Learning_Classifier_DL_detection_*.pgdf'</span>)
</code>
	</pre>
	<p>The predicitons for each class (in this case the classes are
		noise and right whale) are easily accessed in the structure via;</p>
	<pre>
		<code class="lang-matlab">
			<span class="hljs-comment">%% access the prediciton form the first detection </span>
predicitons = dldetections(<span class="hljs-number">1</span>).annotations.dlclassification(<span
				class="hljs-built_in">j</span>).predictions;
</code>
	</pre>
	<p>The loaded detections can then be plotted by accessing the
		waveform data in each structure;</p>
	<pre>
		<code class="lang-matlab">
<span class="hljs-comment">% plot all the spectrograms.</span>
clf
tiledlayout(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>)
<span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span>=<span
				class="hljs-number">1</span>:<span class="hljs-built_in">length</span>(dldetections)

    nexttile

    <span class="hljs-comment">% generate the data for a spectrgram</span>
    [s, w, t] = spectrogram(dldetections(<span class="hljs-built_in">i</span>).wave,<span
				class="hljs-number">512</span>, <span class="hljs-number">384</span>,[],sR,<span
				class="hljs-string">'yaxis'</span>);

    <span class="hljs-comment">% create the time and frequency matrices required to plot a surface </span>
    [X, Y] = <span class="hljs-built_in">meshgrid</span>(t,w);
    <span class="hljs-comment">% plot the surface (divide and multiply by 1000 to show milliseconds and kHz respectively)</span>
    surf(X*<span class="hljs-number">1000</span>, Y/<span
				class="hljs-number">1000</span>, <span class="hljs-number">20</span>*<span
				class="hljs-built_in">log10</span>(<span class="hljs-built_in">abs</span>(s))<span
				class="hljs-number">-140</span>, <span class="hljs-string">'EdgeColor'</span>, <span
				class="hljs-string">'None'</span>)
    view([<span class="hljs-number">0</span>,<span class="hljs-number">90</span>])

    caxis([<span class="hljs-number">70</span>, <span
				class="hljs-number">140</span>]<span class="hljs-number">-140</span>)  
    ylim([<span class="hljs-number">0</span>,<span class="hljs-number">0.5</span>]); 
    xlabel(<span class="hljs-string">''</span>)
    ylabel(<span class="hljs-string">''</span>)

    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">mod</span>(<span
				class="hljs-built_in">i</span>,<span class="hljs-number">5</span>)==<span
				class="hljs-number">0</span>)
       c = colorbar;  
       c.Label.String = <span class="hljs-string">'Amplitude (dB)'</span>; 
    <span class="hljs-keyword">end</span>

    <span class="hljs-comment">%x axis only on bottom plots</span>
    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">i</span>&gt;=<span
				class="hljs-number">20</span>)
       xlabel(<span class="hljs-string">'Time (ms)'</span>) 
    <span class="hljs-keyword">end</span>

    <span class="hljs-comment">%y axis only on left most plots</span>
    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">mod</span>(<span
				class="hljs-built_in">i</span><span class="hljs-number">-1</span>,<span
				class="hljs-number">5</span>)==<span class="hljs-number">0</span>)
       ylabel(<span class="hljs-string">'Frequency (kHz)'</span>) 
    <span class="hljs-keyword">end</span>
</code>
	</pre>
	<p align="center">
		<img width="800" height="700"
			src="images/right_whale_detection_MATLAB.png">
	</p>

	<p>
		<em>Right whale detections from a deep learning model imported
			and then plotted in MATLAB</em>
	</p>
	<h3 id="r">R</h3>
	<p>In the same way as MATLAB export, the PAMGuard time base display
		and export selected data units directly to an R struct which can be
		imported easily into R..</p>
	<p>
		R also has a well supported PAMGuard library with like for like
		functions compared to the MATLAB library. The PAMBinaries R library
		can be found <a href="https://github.com/TaikiSan21/PamBinaries">here</a>.
	</p>
	<h2 id="common-bugs-and-mistakes">Common bugs and mistakes</h2>
	<p>You should always have deep learning models in their own folder.
		Do not have any additional jar files or other programming related
		things (like .dll files) in the same or sub folders. This has been
		known to cause issues with loading models which we have not got to the
		bottom of yet.</p>
	<p>Pytorch models must be saved using jit to be compatible with
		PAMGuard.</p>
	<p>Tensorflow models must be saved as .pb files to be opened in
		PAMGuard.</p>
</body>
</html>